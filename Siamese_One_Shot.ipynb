{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Siamese_One_Shot.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9oQLnqhucl5i",
        "LWFewDVDcs-e",
        "A71TaVguc5rE",
        "JYI36b-LdZAW",
        "mrqB27R_dRpU",
        "JxLYBtF6deGN",
        "j-bDC1KWdl4w",
        "PM8S7J_Bdn97",
        "rWhlT5kaddOq",
        "mEkS_OSges0T",
        "eQ2-ATAcegMH",
        "cmNiFe5Cd-AZ",
        "I0C6Zzk3eD6E"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ivovs65/Siamese_One_Shot_Reproduction/blob/master/Siamese_One_Shot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlVVkYQchd1N",
        "colab_type": "text"
      },
      "source": [
        "# **Siamese Neural Networks for One-Shot Image Recognition** \n",
        "---\n",
        "Bishwas Regmi, 4467655 b.regmi@student.tudelft.nl\n",
        "\n",
        "Daan Boeke, 4471539 d.boeke@student.tudelft.nl\n",
        "\n",
        "Ivo van Straalen, 4467531, i.vanstraalen@student.tudelft.nl\n",
        "\n",
        "<img src='Figures/Figure1.PNG' width=\"300\"/>\n",
        "\n",
        "## Introduction\n",
        "One of the biggest bottlenecks in machine learning is the acquisition of training data. Humans are able to quickly recognize variants of previously seen data, while machines need a lot of (labelled) training examples to make accurate predictions. One way of combatting this is One-Shot Learning (Fei-Fei et al., 2006, Lake et al., 2011), where the model needs to predict the class of a given sample by comparing it to a single representative of each or some of the classes. In this post we reproduce the results of Koch et al. 2015 ([*Siamese Neural-Networks of One-Shot Image Recognition*](https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf)), in which a Siamese Neural Network is utilized and trained for a one-shot task on the Omniglot dataset. The main goal of this post is to reproduce the results of the verification task by Koch et al. This reproduction is part of the course CS4240 Deep Learning at the TUDelft, where this reproduced paper is one of the many as presented in https://reproducedpapers.org/  . In addition to this, the one-shot results are also reproduced. A lot of different implementations are available online by indepedent authors, however, they either only focus on the one-shot task, are written in a different framework than pytorch, or are missing important details from the paper, thus a lot of the code was written from scratch. The code is given below for the interested.\n",
        "\n",
        "## Method and Architecture\n",
        "The goal of the model is to predict whether two samples from the Omniglot data set are variations of the same character. \n",
        "The model used to attain this behaviour is a Siamese Neural Network (Bromley et al., 1993). A siamese neural network consists of two branches of twin networks with tied weights, meaning the two networks share the same weights and structure, and are combined into some other network. This means a siamese neural network has two distinct inputs and produces one output. In this case the network accepts two images from the Omniglot dataset of 105x105 pixels and outputs a single number, representing the probability the two images come from the same class. The detailed architecture used can be seen in the figure below. It is important to remember that the two networks before fusion are in reality the same network used twice. In summary: the first part of the network consists of 4 convolutional layers, followed by a fully connected layer. This is done for both images, after which the two results are combined by computing the element-wise L1-distance between the two vectors. Finally, this vector is then used as the input of the final fully connected layer resulting in the output of the network. \n",
        "\n",
        "![Figure 1](Figures/real_siamese.png)\n",
        "\n",
        "**Loss Function** \n",
        "\n",
        "The loss function used to train this network is a binary cross-entropy loss function with L2-regularization given by: \n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "  \\mathcal{L}(x_1, x_2) = y(x_1, x_2) \\cdot log(p(x_1, x_2)) + (1-y(x_1, x_2)) \\cdot log(1-p(x_1, x_2)) + \\lambda^T |{w}|^2.\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "Here y represents the label of the image pair, which is 1 when the two are from the same character class and 0 otherwise; p is the output of the network and represents the prediction, w is the vector of network weights and labda is the regularization constant. Adam was used as optimizer. The convolutional layers were initialized with $\\mathcal{N}$($\\mu$, $\\sigma$) = $\\mathcal{N}(0, 10^{-2}$) for the weights and $\\mathcal{N}(0.5, 10^{-2})$ for the biases. The fully connected layers were initialized with $\\mathcal{N}(0.5, 10^{-1})$ for the weights and $\\mathcal{N}(0.5, 10^{-2})$ for the biases. \n",
        "\n",
        "## Dataset\n",
        "The dataset that the model was trained on is Omniglot dataset. It consists of images of characters from 50 different alphabhets. Each alphabet has 15-40 characters and each character has 20 different handwritten samples. The omniglot dataset is divided into 'images_background' which consists of 30 of the 50 alphabets and 'images_evaluation' which contains the rest of the 20 alphabets.  \n",
        "\n",
        "**Training, Validation and Test Sets** \n",
        "\n",
        "The training , validation and test sets ('TrainSet', 'EvalSet' and 'OneShotTestSet') are made as described in the paper. For TrainSet, 60% of the first 30 alphabets is used. This means that 12 out of the 20 handdrawn samples were chosen for each character of the 30 alphabets.  From these samples, 30k, 90k and 150k pairs were made randomly, while ensuring that the 50% of the pairs were of the same character.  For EvalSet, 40% of the character samples were chosen from the first 10 alphabets in the 'images_evaluation' folder. From these samples, 10k random pairs were made. For OneShotTestSet, 40% of the characters from the remaining 10 alphabets were chosen. An additional validation set called 'OneShotValSet' is also created from the same samples as in the EvalSet. This OneShotValSet is used to perform One-Shot validation after each epoch. The One-Shot validation error is used for early-stopping of the training, i.e., the training will stop if the One-Shot validation error does not decrease for 20 consequitive epochs.\n",
        "\n",
        "## Experiments\n",
        "The network is trained separately on 30k, 90k, 150k, 30k x 9 (affine distortions, 90k x 9 (affine distortions) and 150k x 9 (affine distortions)  pairs of images.  For training, Adam optimizer is used, which optimizes the Binary Cross Entropy loss between the output of the model and the ground truth. The training is done on batches, where each batch is 128 pairs of images. The model is trained for a maximum of 200 epochs. It can be stopped early if the the One-Shot validation error does not decrease for 20 consequitive epochs. There are checkpoints saved every 10 epochs. These checkpoints are used to resume the training if it was interrupted before completion. Also, each time the One-Shot validation error decreases, a checkpoint called 'best.pt' is saved. This checkpoint is later used for the final One-Shot Test. After the network is fully trained, the 'best.pt' checkpoint is loaded and One-Shot Test is performed on the model.\n",
        "\n",
        "## Results\n",
        "The best achieved validation accuracies for each of the different training set sizes are summarized in the table below.\n",
        "The Table shows the corresponding accuracies reported by the original paper in the second column and our reproduced accuracies in the last column. The reproduced accuracies for the training sets without affine transformations present resemble the paper accuracies closely, only differing by at most 0.14%, which could simply be explained by the difference in (random) sampling. The training sets with affine transformations all show a significant increase in accuracy compared to the original reported accuracy, up to about 1.5% for the 90k with affine transformations set. This might indicate that our approach for adding affine transformations differs from the paper. However, as we achieve similar or greater accuracies, we consider the papers verification results reproduced.\n",
        "\n",
        "<img src='Figures/Table1_updated.PNG' width=\"300\"/>\n",
        "\n",
        "## Discussion and analysis\n",
        "When working on this reproducibility project, we tried to follow the paper's descriptions on the several procedures as closely as possible. However, in some cases unclarity or lack of information was leading to discussion points.\n",
        "\n",
        "**Dataset creation** \n",
        "\n",
        "We found that the paper left a room for interpretation when it defined the dataset splitting procedure. The procedure led to some confusion concerning the validation set. As we initially thought we had to create the validation set from the 8 drawers left unused from the training data. When we ran the experiment, we ended up with accuracies reaching up to 97.8% using the largest training set. As this was significantly above the stated accuracy, we then figured that we had to use 10 alphabets out of the 20 alphabets from the evaluation folder. Using these led to the achieved reported results, which more closely resembled the papers accuracies. However, we are still in doubt if we might misunderstood the authors mentioned data splitting. \n",
        "\n",
        "**Optimization** \n",
        "\n",
        "The paper used the beta version of a Bayesian optimization framework called Whetlab to optimize the hyperparameters. However, at the time of reproduction this framework was not publicly available anymore and not all the optimization details were given in the paper. Therefore, we have used the Adam optimizer instead as this is easy to use and often very reliable. We tried to incoperate as much of the information that was available to resemble the original paper as closely as possible. \n",
        "\n",
        "**Affine transformations** \n",
        "\n",
        " As mentioned in the results, we achieved considerably better results for the training sets which include affine transformations compared to the original paper. We noticed that when using the prescribed parameter ranges for the affine transformation components, the characters were partially moved outside of the frame occasionally. In our code we prevent this from happening to prevent losing any image information. It could be possible that the authors of the original paper did not constrain the transformation in this manner, which could have let them to lose some valuable information on these images and therefore lose accuracy. Naturally, it could also be that our approach of adding transformations to the training set was incorrect leading to a positive bias in our resulting accuracies.\n",
        "\n",
        "## Analysis\n",
        "In addition to reproducing the requested verification Table, we also did some additional analysis.\n",
        "\n",
        "**One-shot error**\n",
        "\n",
        " After that we obtained our models, we were interested in seeing how well they would perform on the main task of the paper: one-shot learning. We implemented the method described in the one-shot learning section and tested all our 6 model on data from the test set. This resulted in the table below.\n",
        "As can been seen, the accuracy steadily increases with increasing training set size and the best model, 150k with affine distortions, almost reaches the papers one shot accuracy of 92%. The difference could be explained by the fact that in our reproduction we averaged 10 runs of the one-shot task, while the paper does not specify anything about this.\n",
        "\n",
        "<img src='Figures/OneShotTable.PNG' width=\"400\"/>\n",
        "**First layer filters**\n",
        "\n",
        " The paper illustrates the convolutional filters present in the first layer of their 150k with transformation model. You can clearly see each filter adapting its own role, most of the filters looking for small points in the input images and some that represent lines to detect edges. We thought it would be interesting to see how the filters in our first layer compare to the ones in the paper and see if our best model learns different features to accomplish the same task. In figure below, the filters from the first convolutional layer are visualized. As can be seen, the filters seem mainly to focus on lines present in the image, with a few focussing on dots. The filters from the paper seem to focus much more on a lot of small dots in each filter, while the reproduced filters focus at most at 2 larger dots. This might be caused by the use of a different optimizer and would be interesting to investigate further. As reference the filters of the first layer given in the paper is shown first.\n",
        "\n",
        "\n",
        "<img src='Figures/FirstLayerPaper.PNG' width=\"650\"/>\n",
        "\n",
        "And our filters:\n",
        "\n",
        "<img src='Figures/KernelWeights.png' width=\"650\"/>\n",
        "\n",
        " **Layer Activations**\n",
        "Although the paper does not show the activations of each layer, we still found it interesting to see how each layer behaved. The activations of each convolutional filter after input of the following character can be seen below. \n",
        "\n",
        "![original](Figures/original.png)\n",
        "\n",
        "The first convolutional layer seems to detect horizontal and vertical edges, which can be seen by the shadows that form around the character. \n",
        "\n",
        "<img src='Figures/layer1.png' width=\"650\"/>\n",
        "\n",
        "In the second convolutional layer is already much harder to see what happens, but it seems to detect structures of strokes, but the third layer also seems to detect on this. For the fourth layer it is almost incomprehensible what it is doing.\n",
        "\n",
        "![Layer2 Act](Figures/layer2.png)\n",
        "![Layer3 Act](Figures/layer3.png)\n",
        "![Layer4 Act](Figures/layer4.png)\n",
        "\n",
        "\n",
        "\n",
        "# References\n",
        "Koch, Gregory, Zemel Richard, and Salakhutdinov, Ruslan. Siamese Neural Networks for One-shot Image Recognition. In *Proceedings of the 32nd International Conference on Machine Learning*, Lille, France, 2015. JMLR: W&CP volume 37. \n",
        "\n",
        "\n",
        "Fei-Fei, Li, Fergus, Robert, and Perona, Pietro. A bayesian approach to unsupervised one-shot learning of object categories. In *Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on,* pp. 1134 - 1141. IEEE, 2003. \n",
        "\n",
        "Lake, Brenden M, Salakhutdinov, Ruslan, Gross, Jason, and Tenenbaum, Joshua B. One shot learning of simple visual concepts. In *Proceedings of the 33rd Annual Conference of the Cognitive Science Society*, volume 172, 2011. \n",
        "\n",
        "Bromley, Jane, Bentz, James W, Bottou, Léon, Guyon, Isabelle, LeCun, Yann, Moore, Cliff, Säckinger, Eduard, and Shah, Roopak. Signature Verification using a siamese time delay neural network. *International Journal of Pattern Recognition and Artificial Intelligence*, 7(04):669-688, 1993\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oQLnqhucl5i",
        "colab_type": "text"
      },
      "source": [
        "# Loading Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf9XjfOrb3Zw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWFewDVDcs-e",
        "colab_type": "text"
      },
      "source": [
        "# Setting Up Access To Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fku5XgJ4PaQw",
        "colab_type": "code",
        "outputId": "7649550a-64c4-402f-8189-0d156ea51dc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "path = '/content'\n",
        "root_path = path\n",
        "os.chdir(path)\n",
        "!pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A71TaVguc5rE",
        "colab_type": "text"
      },
      "source": [
        "# The Siamese Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxV17P4-PaQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initWeights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.normal_(m.weight, mean=0, std=0.2)\n",
        "        m.bias.data.normal_(mean = 0.5, std = 0.01)\n",
        "    elif type(m) == nn.Conv2d:\n",
        "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
        "        m.bias.data.normal_(mean = 0.5, std = 0.01) \n",
        "\n",
        "class SiameseNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=10, padding=0, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=7, padding=0, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 128, kernel_size=4, padding=0, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, padding=0, stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        \n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(256*6*6, 4096), \n",
        "            nn.Sigmoid())\n",
        "        \n",
        "        self.final = nn.Sequential(\n",
        "            nn.Linear(4096, 1))\n",
        "        \n",
        "        self.apply(initWeights)\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        x = self.layer2(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        out1 = self.forward_one(x1)\n",
        "        out2 = self.forward_one(x2)\n",
        "        dis = torch.abs(out1 - out2)\n",
        "        out = self.final(dis) \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYI36b-LdZAW",
        "colab_type": "text"
      },
      "source": [
        "# Checking the Structure of the Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM65cQZPPaRL",
        "colab_type": "code",
        "outputId": "ad314df6-624c-4abe-ec5e-8964f4ed4f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "def test(net):\n",
        "    total_params = 0\n",
        "\n",
        "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
        "        total_params += np.prod(x.data.numpy().shape)\n",
        "    print(\"Total number of params\", total_params)\n",
        "    print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
        "\n",
        "\n",
        "net = SiameseNet()\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "for name, param in net.named_parameters():\n",
        "    print(name)\n",
        "\n",
        "test(net)\n",
        "if use_gpu:\n",
        "    net = net.cuda()\n",
        "summary(net, input_size=[(1,105,105), (1,105,105)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "layer1.0.weight\n",
            "layer1.0.bias\n",
            "layer1.3.weight\n",
            "layer1.3.bias\n",
            "layer1.6.weight\n",
            "layer1.6.bias\n",
            "layer1.9.weight\n",
            "layer1.9.bias\n",
            "layer2.0.weight\n",
            "layer2.0.bias\n",
            "final.0.weight\n",
            "final.0.bias\n",
            "Total number of params 38951745\n",
            "Total layers 6\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 96, 96]           6,464\n",
            "              ReLU-2           [-1, 64, 96, 96]               0\n",
            "         MaxPool2d-3           [-1, 64, 48, 48]               0\n",
            "            Conv2d-4          [-1, 128, 42, 42]         401,536\n",
            "              ReLU-5          [-1, 128, 42, 42]               0\n",
            "         MaxPool2d-6          [-1, 128, 21, 21]               0\n",
            "            Conv2d-7          [-1, 128, 18, 18]         262,272\n",
            "              ReLU-8          [-1, 128, 18, 18]               0\n",
            "         MaxPool2d-9            [-1, 128, 9, 9]               0\n",
            "           Conv2d-10            [-1, 256, 6, 6]         524,544\n",
            "             ReLU-11            [-1, 256, 6, 6]               0\n",
            "           Linear-12                 [-1, 4096]      37,752,832\n",
            "          Sigmoid-13                 [-1, 4096]               0\n",
            "           Conv2d-14           [-1, 64, 96, 96]           6,464\n",
            "             ReLU-15           [-1, 64, 96, 96]               0\n",
            "        MaxPool2d-16           [-1, 64, 48, 48]               0\n",
            "           Conv2d-17          [-1, 128, 42, 42]         401,536\n",
            "             ReLU-18          [-1, 128, 42, 42]               0\n",
            "        MaxPool2d-19          [-1, 128, 21, 21]               0\n",
            "           Conv2d-20          [-1, 128, 18, 18]         262,272\n",
            "             ReLU-21          [-1, 128, 18, 18]               0\n",
            "        MaxPool2d-22            [-1, 128, 9, 9]               0\n",
            "           Conv2d-23            [-1, 256, 6, 6]         524,544\n",
            "             ReLU-24            [-1, 256, 6, 6]               0\n",
            "           Linear-25                 [-1, 4096]      37,752,832\n",
            "          Sigmoid-26                 [-1, 4096]               0\n",
            "           Linear-27                    [-1, 1]           4,097\n",
            "================================================================\n",
            "Total params: 77,899,393\n",
            "Trainable params: 77,899,393\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 463.68\n",
            "Forward/backward pass size (MB): 29.83\n",
            "Params size (MB): 297.16\n",
            "Estimated Total Size (MB): 790.67\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrqB27R_dRpU",
        "colab_type": "text"
      },
      "source": [
        "# Hyperparameter Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBXeCbI5PaRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseNetArgs:\n",
        "    def __init__(self, momentum=0.1, lr=0.1,\n",
        "                 lr_decay=0.99, reg=0.1, batch_size=128, epochs=200, start_epoch=0,\n",
        "                 save_every=10,\n",
        "                 temp_path='temp_save', best_path='save'):\n",
        "        self.batch_size = batch_size\n",
        "        self.start_epoch = start_epoch\n",
        "        self.epochs = epochs\n",
        "        self.save_every = save_every\n",
        "        self.temp_path = temp_path\n",
        "        self.best_path = best_path\n",
        "        #self.m_rate = [(0.5 - i) / self.epochs for i in momentum]\n",
        "        self.momentum = momentum #[0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
        "        self.lr = lr  # Learning rate\n",
        "        self.lr_decay = lr_decay\n",
        "        self.reg = reg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxLYBtF6deGN",
        "colab_type": "text"
      },
      "source": [
        "# The Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0msZfbhPaRV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    dataiter = iter(train_loader)\n",
        "    err = 0\n",
        "    for i in range(0, int(len(train_loader))):\n",
        "        img1, img2, target = dataiter.__next__()\n",
        "        \n",
        "        if torch.cuda.is_available():\n",
        "            img1, img2, target = img1.cuda(), img2.cuda(), target.cuda()\n",
        "\n",
        "        img1 = torch.autograd.Variable(img1)\n",
        "        img2 = torch.autograd.Variable(img2)\n",
        "        target = torch.autograd.Variable(target)\n",
        "        output = model.forward(img1, img2)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = output.T[0]\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        err += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        \n",
        "        if (i%200)==0: \n",
        "          print(\"In epoch: \", epoch, \" batch: \", i, \" loss =\", loss.item()) \n",
        "\n",
        "    return err\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-bDC1KWdl4w",
        "colab_type": "text"
      },
      "source": [
        "# The Regular Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V32aqSkxdilK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(eval_loader, model):\n",
        "    model.eval()\n",
        "    dataiter = iter(eval_loader)\n",
        "    errors = 0\n",
        "    for i in range(0, int(len(eval_loader))):\n",
        "        img1, img2, target = dataiter.__next__()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            img1, img2, target = img1.cuda(), img2.cuda(), target.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.forward(img1, img2)\n",
        "        output =  output.T[0] \n",
        "        output = torch.sigmoid(output)\n",
        "        errors = errors + (output.round().int() !=  target.int()).int().sum() \n",
        "\n",
        "    error = errors / (128.0 * int(len(eval_loader)))\n",
        "    return error.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM8S7J_Bdn97",
        "colab_type": "text"
      },
      "source": [
        "# The One-Shot Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eajMHuodh_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def oneshot(oneshot_loader, model):\n",
        "    model.eval()\n",
        "    dataiter = iter(oneshot_loader)\n",
        "    errors = 0\n",
        "    for i in range(0, int(len(oneshot_loader))):\n",
        "        img1, img2 = dataiter.__next__()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            img1, img2 = img1.cuda(), img2.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model.forward(img1, img2)\n",
        "        output =  output.T[0] \n",
        "        \n",
        "        output = torch.sigmoid(output)\n",
        "        pred = np.argmax(output.cpu().numpy())\n",
        "        if pred != 0:\n",
        "          errors+=1\n",
        "\n",
        "    error = errors / (int(len(oneshot_loader)))\n",
        "    return error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWhlT5kaddOq",
        "colab_type": "text"
      },
      "source": [
        "# Affine Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzXDy4tm_nnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def AffineTransform(x, n=1, theta=[-10, 10], tx=[-2, 2], ty=[-2, 2], px=[-0.3, 0.3], py=[-0.3, 0.3], sx=[0.8, 1.2],\n",
        "                    sy=[0.8, 1.2]):\n",
        "\n",
        "    # Test whether a constant value is given\n",
        "    input_list = [theta, tx, ty, px, py, sx, sy]\n",
        "    int_ind = [i[0] for i in enumerate(input_list) if isinstance(i[1], int)]\n",
        "    if len(int_ind) > 0:\n",
        "        for j in enumerate(int_ind):\n",
        "            input_list[j[1]] = [input_list[j[1]], input_list[j[1]]]\n",
        "\n",
        "    # Initialize array to store n transformed images in\n",
        "    x_transimg = []\n",
        "\n",
        "    # Creating n transformed images\n",
        "    for k in range(n):\n",
        "        x_trans = [[False, True],[False, True]]\n",
        "        while np.any(~np.array(x_trans)[0, :]) or np.any(~np.array(x_trans)[:, 0]) or np.any(\n",
        "                ~np.array(x_trans)[-1, :]) or np.any(~np.array(x_trans)[:, -1]):\n",
        "\n",
        "            # Translation matrix\n",
        "            transmat = np.identity(3)\n",
        "            tx_temp = np.random.randint(2) * np.random.uniform(input_list[1][0], input_list[1][1])\n",
        "            ty_temp = np.random.randint(2) * np.random.uniform(input_list[2][0], input_list[2][1])\n",
        "            transmat[2, 0] = tx_temp\n",
        "            transmat[2, 1] = ty_temp\n",
        "\n",
        "            # Rotation matrix\n",
        "            rotmat = np.identity(3)\n",
        "            theta_temp = np.random.randint(2) * np.random.uniform(input_list[0][0]*np.pi/180, input_list[0][1]*np.pi/180)\n",
        "            rotmat[0, 0] = np.cos(theta_temp)\n",
        "            rotmat[0, 1] = np.sin(theta_temp)\n",
        "            rotmat[1, 0] = -np.sin(theta_temp)\n",
        "            rotmat[1, 1] = np.cos(theta_temp)\n",
        "\n",
        "            # Scaling matrix\n",
        "            scalemat = np.identity(3)\n",
        "            sx_temp = [1, np.random.uniform(input_list[5][0], input_list[5][1])][np.random.randint(2)]\n",
        "            sy_temp = [1, np.random.uniform(input_list[6][0], input_list[6][1])][np.random.randint(2)]\n",
        "            scalemat[0, 0] = sx_temp\n",
        "            scalemat[1, 1] = sy_temp\n",
        "\n",
        "            # Shearing matrix\n",
        "            shearmat = np.identity(3)\n",
        "            px_temp = np.random.randint(2) * np.random.uniform(input_list[3][0], input_list[3][1])\n",
        "            py_temp = np.random.randint(2) * np.random.uniform(input_list[4][0], input_list[4][1])\n",
        "            shearmat[1, 0] = px_temp\n",
        "            shearmat[0, 1] = py_temp\n",
        "\n",
        "            # Total transformation matrix\n",
        "            totalmat = transmat @ rotmat @ scalemat @ shearmat\n",
        "\n",
        "            # Take the inverse\n",
        "            totalmat_inv = np.linalg.inv(totalmat)\n",
        "            x_trans = x.transform(x.size, Image.AFFINE, data=totalmat_inv.flatten()[:6], resample=Image.BILINEAR, fillcolor='white')\n",
        "        x_transimg.append(x_trans)\n",
        "    return x_transimg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEkS_OSges0T",
        "colab_type": "text"
      },
      "source": [
        "# Create Seperate Folders for the Training and Validation Sets\n",
        "Used to make the loading of the samples into datasets and dataloaders easier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gk1jZ9kxPaRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createTrainAndEvalFolder(root_folder, remake=False):\n",
        "    train_folder = os.path.join(root_folder, 'TrainImages')\n",
        "    eval_folder = os.path.join(root_folder, 'EvaluationImages')\n",
        "    test_folder = os.path.join(root_folder, 'TestImages')\n",
        "    background_folder = os.path.join(root_folder, 'images_background')\n",
        "    evaluation_folder = os.path.join(root_folder, 'images_evaluation')\n",
        "\n",
        "    make = False\n",
        "    if not os.path.exists(train_folder):\n",
        "        os.mkdir(train_folder)\n",
        "        make = True\n",
        "    elif remake:\n",
        "        print(\"Deleting Train Folder\")\n",
        "        shutil.rmtree(train_folder)\n",
        "        print(\"Creating Train Folder\")\n",
        "        os.mkdir(train_folder)\n",
        "\n",
        "    if not os.path.exists(eval_folder):\n",
        "        os.mkdir(eval_folder)\n",
        "        make = True\n",
        "    elif remake:\n",
        "        print(\"Deleting Evaluation Folder\")\n",
        "        shutil.rmtree(eval_folder)\n",
        "        print(\"Creating Evaluation Folder\")\n",
        "        os.mkdir(eval_folder)\n",
        "\n",
        "    if not os.path.exists(test_folder):\n",
        "        os.mkdir(test_folder)\n",
        "        make = True\n",
        "    elif remake:\n",
        "        print(\"Deleting Test Folder\")\n",
        "        shutil.rmtree(test_folder)\n",
        "        print(\"Creating Test Folder\")\n",
        "        os.mkdir(test_folder)\n",
        "\n",
        "    make = make or remake\n",
        "\n",
        "    if make:\n",
        "        print(\"Copying images from origin folder to training, evaluation and test Folder.\")\n",
        "    else:\n",
        "        print(\"Data already present. Only counting number of characters.\")\n",
        "\n",
        "    char_per_alp_train = []\n",
        "    for alphabet in os.listdir(background_folder):\n",
        "        alph_folder = os.path.join(background_folder, alphabet)\n",
        "        a_dest = alphabet\n",
        "        count = 0\n",
        "        if make:\n",
        "            print(\"Copying folder: \" + alph_folder)\n",
        "\n",
        "        for charf in os.listdir(alph_folder):\n",
        "            count = count + 1\n",
        "            char_folder = os.path.join(alph_folder, charf)\n",
        "\n",
        "            number = 0\n",
        "            for character in os.listdir(char_folder):\n",
        "                char = os.path.join(char_folder, character)\n",
        "                if (number < 12):\n",
        "                    dest = os.path.join(train_folder, a_dest)\n",
        "                else :\n",
        "                    break\n",
        "\n",
        "                if not os.path.exists(dest):\n",
        "                    os.mkdir(dest)\n",
        "\n",
        "                dest = os.path.join(dest, charf)\n",
        "                if not os.path.exists(dest) and make:\n",
        "                    os.mkdir(dest)\n",
        "\n",
        "                if make:\n",
        "                    dest = os.path.join(dest, character)\n",
        "                    shutil.copyfile(char, dest)\n",
        "\n",
        "                number = number + 1\n",
        "\n",
        "        char_per_alp_train.append(count)\n",
        "\n",
        "    char_per_alp_eval = []\n",
        "    char_per_alp_test = [] \n",
        "    alphabet_nr = 0\n",
        "    for alphabet in os.listdir(evaluation_folder):\n",
        "        alph_folder = os.path.join(evaluation_folder, alphabet)\n",
        "        a_dest = alphabet\n",
        "        count_eval = 0\n",
        "        count_test = 0\n",
        "        if make:\n",
        "            print(\"Copying folder: \" + alph_folder)\n",
        "\n",
        "        for charf in os.listdir(alph_folder):\n",
        "            if alphabet_nr < 10 :\n",
        "              count_eval = count_eval + 1\n",
        "            else:\n",
        "              count_test = count_test + 1\n",
        "            \n",
        "            char_folder = os.path.join(alph_folder, charf)\n",
        "\n",
        "            number = 0\n",
        "            for character in os.listdir(char_folder):\n",
        "                char = os.path.join(char_folder, character)\n",
        "                if (alphabet_nr < 10 and number < 4):\n",
        "                    dest = os.path.join(eval_folder, a_dest)\n",
        "                elif (alphabet_nr >= 10 and number < 4):\n",
        "                    dest = os.path.join(test_folder, a_dest)\n",
        "                else : \n",
        "                    break\n",
        "\n",
        "                if not os.path.exists(dest):\n",
        "                    os.mkdir(dest)\n",
        "\n",
        "                dest = os.path.join(dest, charf)\n",
        "                if not os.path.exists(dest) and make:\n",
        "                    os.mkdir(dest)\n",
        "\n",
        "                if make:\n",
        "                    dest = os.path.join(dest, character)\n",
        "                    shutil.copyfile(char, dest)\n",
        "\n",
        "                number = number + 1\n",
        "        \n",
        "        if alphabet_nr < 10 :\n",
        "              char_per_alp_eval.append(count_eval)\n",
        "        else :\n",
        "              char_per_alp_test.append(count_test)\n",
        "        alphabet_nr = alphabet_nr + 1\n",
        "\n",
        "    return char_per_alp_train, char_per_alp_eval, char_per_alp_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ2-ATAcegMH",
        "colab_type": "text"
      },
      "source": [
        "# Dataset For Training and Regular Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwxXYF7BPaRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseNetworkDataset(Dataset):\n",
        "\n",
        "    def createSubDataSet(self, dataset, label):\n",
        "        idx = np.where([i == label for i in dataset.targets])[0]\n",
        "        return Subset(dataset, idx)\n",
        "\n",
        "    def __init__(self, folder, alphabet_counts, num_per_char, num_pairs, distort=False, num_dist=8):\n",
        "        self.num_pairs = num_pairs\n",
        "        self.distort = distort\n",
        "        self.num_dist = num_dist\n",
        "\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.ToTensor()])\n",
        "\n",
        "        label = 0\n",
        "        self.imgs = []\n",
        "        for alphabet in os.listdir(folder):\n",
        "            print(\"Loading Alphabet:\", alphabet)\n",
        "            alph_folder = os.path.join(folder, alphabet)\n",
        "            for charf in os.listdir(alph_folder):\n",
        "                alph_array = []\n",
        "                char_folder = os.path.join(alph_folder, charf)\n",
        "\n",
        "                for character in os.listdir(char_folder):\n",
        "                    char = os.path.join(char_folder, character)\n",
        "                    img_i = Image.open(char,'r').convert('L')\n",
        "                    img_t = transform(img_i)\n",
        "                    alph_array.append((img_t, label))\n",
        "\n",
        "                    if self.distort:\n",
        "                        dis_img = AffineTransform(img_i, n=num_dist)\n",
        "                        for i in range(num_dist):\n",
        "                            dis_img_t = transform(dis_img[i])\n",
        "                            alph_array.append((dis_img_t, label))\n",
        "                  \n",
        "                self.imgs.append(alph_array)\n",
        "                label += 1\n",
        "\n",
        "\n",
        "        self.imgindex = []\n",
        "        self.numclasses = label\n",
        "        print('Number of Classes:', self.numclasses)\n",
        "        for i in range(num_pairs):\n",
        "            rand_class1 = np.random.randint(self.numclasses)\n",
        "            r1 = np.random.choice(range(0,self.imgs[rand_class1].__len__(),(self.distort*self.num_dist+1)))\n",
        "            same_class = np.random.randint(2)\n",
        "            if same_class:\n",
        "                rand_class2 = rand_class1\n",
        "                r2 = np.random.choice(range(0,self.imgs[rand_class2].__len__(),(self.distort*self.num_dist+1)))\n",
        "            else:\n",
        "                rand_class2 = np.random.choice(\n",
        "                    [i for i in range(0, rand_class1)] + [i for i in range(rand_class1 + 1, self.numclasses)])\n",
        "                r2 = np.random.choice(range(0,self.imgs[rand_class2].__len__(),(self.distort*self.num_dist+1)))\n",
        "            \n",
        "            self.imgindex.append((rand_class1, r1, rand_class2, r2))\n",
        "            if self.distort:\n",
        "                for i in range(1,self.num_dist+1):\n",
        "                    self.imgindex.append((rand_class1, r1+i, rand_class2, r2+i))\n",
        "            \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        class1, idx1, class2, idx2 = self.imgindex[index]\n",
        "        img1, label1 = self.imgs[class1][idx1]\n",
        "        img2, label2 = self.imgs[class2][idx2]\n",
        "        label = label1 == label2\n",
        "        return img1, img2, torch.tensor(label, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.distort:\n",
        "            length = self.num_pairs*(self.num_dist+1)\n",
        "        else:\n",
        "            length = self.num_pairs\n",
        "        return length\n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmNiFe5Cd-AZ",
        "colab_type": "text"
      },
      "source": [
        "# Dataset for the One-Shot Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5R_3uEVd88J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SiameseOneShotDataSet(Dataset):\n",
        "\n",
        "    def __init__(self, folder, alphabet_counts, num_per_char, num_pairs=30000):\n",
        "        self.num_pairs = num_pairs\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.ToTensor()])\n",
        "\n",
        "        label = 0\n",
        "        self.imgs = []\n",
        "        for alphabet in os.listdir(folder):\n",
        "            alph_folder = os.path.join(folder, alphabet)\n",
        "            for charf in os.listdir(alph_folder):\n",
        "                char_folder = os.path.join(alph_folder, charf)\n",
        "                alph_array = []\n",
        "\n",
        "                for character in os.listdir(char_folder):\n",
        "                    char = os.path.join(char_folder, character)\n",
        "                    img = Image.open(char, 'r').convert('L')\n",
        "                    img = transform(img)\n",
        "                    alph_array.append((img, label))\n",
        "\n",
        "                self.imgs.append(alph_array)\n",
        "                label += 1\n",
        "        \n",
        "        self.numclasses = label\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        idx = index % 20\n",
        "        label = None\n",
        "        # generate image pair from same class\n",
        "        if idx == 0:\n",
        "            self.c1 = np.random.randint(self.numclasses)\n",
        "            r1 = np.random.randint(self.imgs[self.c1].__len__())\n",
        "            self.img1, _ = self.imgs[self.c1][r1]\n",
        "            r2 = np.random.randint(self.imgs[self.c1].__len__())\n",
        "            img2, _ = self.imgs[self.c1][r2]\n",
        "        # generate image pair from different class\n",
        "        else:\n",
        "            rand_class2 = np.random.choice([i for i in range(0, self.c1)] + [i for i in range(self.c1 + 1, self.numclasses)])\n",
        "            r2 = np.random.randint(self.imgs[rand_class2].__len__())\n",
        "            img2, _ = self.imgs[rand_class2][r2]\n",
        "\n",
        "        return self.img1, img2\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_pairs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0C6Zzk3eD6E",
        "colab_type": "text"
      },
      "source": [
        "# Loading All the Data\n",
        "Function used to load the different datasets from their folders into the corresponding dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpwDDABXd6nR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(num_samples, one_shot=False, remake=False, distort=False):\n",
        "    # First check whether the omniglot dataset is already downloaded\n",
        "    p = os.path.join(path, 'data')\n",
        "    q = os.path.join(p, 'images_background')\n",
        "    s = os.path.join(q, 'images_evaluation')\n",
        "    if not os.path.exists(p) or not os.path.exists(q) or not os.path.exists(s):\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.ToTensor(), transforms.Grayscale()])\n",
        "        torchvision.datasets.omniglot.Omniglot(root=p, background=True,\n",
        "                                               download=True, transform=transform)\n",
        "        torchvision.datasets.omniglot.Omniglot(root=p, background=False,\n",
        "                                               download=True, transform=transform)\n",
        "\n",
        "    # Create folders for training and evaluation\n",
        "    root_folder = os.path.join(p,'omniglot-py')#'./data/omniglot-py'\n",
        "    alphabet_counts_train, alphabet_counts_eval, alphabet_counts_test  = createTrainAndEvalFolder(root_folder, remake)\n",
        "    print(alphabet_counts_train)\n",
        "    print(alphabet_counts_eval)\n",
        "    print(alphabet_counts_test)\n",
        "    train_weights = np.array([])\n",
        "    eval_weights = np.array([])\n",
        "    test_weights = np.array([])\n",
        "    for i in alphabet_counts_train:\n",
        "        train_weights = np.append(train_weights, 1 / (12 * i) * np.ones(12 * i)) \n",
        "    for i in alphabet_counts_eval:\n",
        "        eval_weights = np.append(eval_weights, 1 / (4 * i) * np.ones(4 * i)) \n",
        "    for i in alphabet_counts_test:\n",
        "        test_weights = np.append(test_weights, 1 / (4 * i) * np.ones(4 * i)) \n",
        "\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.Grayscale(num_output_channels=1), transforms.ToTensor()])\n",
        "    TrainSet = SiameseNetworkDataset(root_folder + '/TrainImages', alphabet_counts_train, 12, num_pairs=num_samples, distort=distort)\n",
        "    EvalSet = SiameseNetworkDataset(root_folder + '/EvaluationImages', alphabet_counts_eval, 4, num_pairs=10000)\n",
        "    OneShotValSet = SiameseOneShotDataSet(root_folder + '/EvaluationImages', 20, 20, num_pairs=320*20)\n",
        "    OneShotTestSet = SiameseOneShotDataSet(root_folder + '/TestImages', 20, 20, num_pairs=400*20)\n",
        "\n",
        "    TrainLoader = torch.utils.data.DataLoader(TrainSet, shuffle=True, batch_size=128, num_workers=4, pin_memory=True)\n",
        "    EvalLoader = torch.utils.data.DataLoader(EvalSet, shuffle=True, batch_size=128, num_workers=4, pin_memory=True)\n",
        "    OneShotValLoader = torch.utils.data.DataLoader(OneShotValSet, batch_size=20, num_workers=4, pin_memory=True)\n",
        "    OneShotTestLoader = torch.utils.data.DataLoader(OneShotTestSet, batch_size=20, num_workers=4, pin_memory=True)\n",
        "    \n",
        "    return TrainLoader, EvalLoader, OneShotValLoader, OneShotTestLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWaV9SDHeG9X",
        "colab_type": "text"
      },
      "source": [
        "# Main\n",
        "Global code for initialization and training the network, one-shot testing and plotting of the convolutional filters and activations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2OgZMLtuFAM7",
        "colab": {}
      },
      "source": [
        "def main(NumPairs=30000, distort=False, save_path='/content/gdrive/My Drive/Siamese_Net/saves/',resume=True):\n",
        "    net = SiameseNet()\n",
        "    net.cuda()\n",
        "    \n",
        "    net.apply(initWeights)\n",
        "    # First Train with 30k Pairs\n",
        "\n",
        "    TrainLoader, EvalLoader, OneShotValLoader, OneShotTestLoader = read_data(NumPairs, one_shot=False, remake=False, distort=distort) \n",
        "    params = SiameseNetArgs(epochs= 200, lr=0.00006, momentum=0.5, save_every=1, temp_path= save_path + \"Checkpoint_\"+str(NumPairs), best_path=save_path+\"Best_\"+str(NumPairs))\n",
        "    \n",
        "    criterion = torch.nn.BCEWithLogitsLoss(reduction='mean')\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr = params.lr)\n",
        "    \n",
        "    epoch = 0\n",
        "    best_epoch_error = 10 \n",
        "    val_errors = []\n",
        "    os_errors = []\n",
        "    avg_loss = []\n",
        "    isbest_counter =0\n",
        "\n",
        "    if not os.path.exists(params.temp_path):\n",
        "        os.mkdir(params.temp_path)\n",
        "    elif resume:\n",
        "        print('Checkpoint Folder already exists. Resuming from checkpoint.')\n",
        "        ch = os.path.join(params.temp_path, 'checkpoint.pt')\n",
        "        checkpoint = torch.load(ch)\n",
        "        \n",
        "        epoch = checkpoint['epoch']\n",
        "        print(\"ckp epoch \", checkpoint['epoch']) \n",
        "        val_err = checkpoint['val_err']\n",
        "        os_err = checkpoint['os_err']\n",
        "        net.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        val_errors = checkpoint['val_errors']\n",
        "        avg_loss = checkpoint['avg_loss']\n",
        "        os_errors = checkpoint['os_errors'] \n",
        "\n",
        "        ch = os.path.join(params.best_path, 'best.pt')\n",
        "        best_checkpoint = torch.load(ch)\n",
        "        best_epoch = best_checkpoint['epoch']\n",
        "        best_epoch_error = best_checkpoint['os_err']\n",
        "\n",
        "        if (epoch-1-best_epoch) >= 20:\n",
        "          return \n",
        "\n",
        "        if (best_epoch>epoch):\n",
        "          isbest_counter = best_epoch - epoch\n",
        "          epoch = best_epoch\n",
        "          print(\"updating ckp epoch to best_ckp epoch \", epoch) \n",
        "          val_err = best_checkpoint['val_err']\n",
        "          os_err = best_checkpoint['os_err']\n",
        "          net.load_state_dict(best_checkpoint['state_dict'])\n",
        "          optimizer.load_state_dict(best_checkpoint['optimizer'])\n",
        "          val_errors = best_checkpoint['val_errors']\n",
        "          avg_loss = best_checkpoint['avg_loss']\n",
        "          os_errors = best_checkpoint['os_errors'] \n",
        "\n",
        "    else: \n",
        "        return # Do One-Shot Test\n",
        "\n",
        "    if not os.path.exists(params.best_path):\n",
        "        os.mkdir(params.best_path)\n",
        "    \n",
        "    while epoch < params.epochs: \n",
        "        # Train \n",
        "        print(\"Epoch: {}. Start Training.\".format(epoch))\n",
        "        train_err = train(TrainLoader, net, criterion=criterion, optimizer=optimizer, epoch=epoch)\n",
        "        batch_loss = train_err/int(len(TrainLoader))\n",
        "        avg_loss.append(batch_loss)\n",
        "        print (\"Total Train Loss = {}.     Avg. Loss/batch = {}\".format(train_err, batch_loss))\n",
        "        \n",
        "        # Validate\n",
        "        print(\"Done Training. Start Evaluation.\")\n",
        "        val_err = validate(EvalLoader, net)\n",
        "        print(\"Epoch: {}, Val Error: {}\".format(epoch, val_err))\n",
        "        val_errors.append(val_err)\n",
        "\n",
        "        # Validate One-Shot\n",
        "        print(\"Start One-Shot Validation.\")\n",
        "        os_err = oneshot(OneShotValLoader, net)\n",
        "        print(\"Epoch: {}, OneShot Error: {}\".format(epoch, os_err))\n",
        "        os_errors.append(os_err)\n",
        "\n",
        "        if epoch == 0:\n",
        "            best_epoch_error = os_err\n",
        "\n",
        "        is_best = os_err < best_epoch_error\n",
        "        best_epoch_error = min(os_err, best_epoch_error) \n",
        "\n",
        "        if epoch % params.save_every == 0:\n",
        "            print(\"Saving Checkpoint at Epoch\", epoch) \n",
        "            fd = open(params.temp_path+'/epoch.txt', 'w')\n",
        "            fd.write(\"Epoch: \" + str(epoch) + \" Val error: \" + str(val_err))\n",
        "            fd.write(\"Epoch: \" + str(epoch) + \" OneShot error: \" + str(os_err))\n",
        "            fd.close() \n",
        "\n",
        "            checkpoint = {\n",
        "            'epoch': epoch + 1,\n",
        "            'os_err': os_err,\n",
        "            'val_err': val_err,\n",
        "            'state_dict': net.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'val_errors' : val_errors,\n",
        "            'avg_loss' : avg_loss,\n",
        "            'os_errors' : os_errors\n",
        "            }\n",
        "            torch.save(checkpoint, os.path.join(params.temp_path, 'checkpoint.pt'))\n",
        "        if is_best:\n",
        "            isbest_counter = 0\n",
        "            best_epoch = epoch\n",
        "            print(\"Saving Best at Epoch {} with OneShot error {os_error:.4f} and val error {error:.4f}\".format(epoch, os_error = best_epoch_error, error=val_err))\n",
        "            os.path.join(params.best_path, 'epoch.txt')\n",
        "            fd = open(params.best_path+'/epoch.txt', 'w')\n",
        "            fd.write(\"Epoch: \" + str(epoch) + \" Val error: \" + str(val_err))\n",
        "            fd.write(\"Epoch: \" + str(epoch) + \" OneShot error: \" + str(os_err))\n",
        "            fd.close() \n",
        "\n",
        "            checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'val_err': val_err,\n",
        "            'os_err': best_epoch_error,\n",
        "            'state_dict': net.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'val_errors' : val_errors,\n",
        "            'avg_loss' : avg_loss,\n",
        "            'os_errors' : os_errors\n",
        "            }\n",
        "            torch.save(checkpoint, os.path.join(params.best_path, 'best.pt'))\n",
        "        else :\n",
        "            isbest_counter = isbest_counter + 1 \n",
        "\n",
        "        if isbest_counter >= 20 :         # early stopping\n",
        "            print(\"Early stopping because One-Shot error didn't improve for last 20 epochs.\")\n",
        "            print(\"Stopping epoch : \", epoch, \"   Best OneShot error \", best_epoch_error, \" in epoch : \", best_epoch)\n",
        "            break; \n",
        "\n",
        "        epoch+=1    \n",
        "\n",
        "\n",
        "    # One Shot\n",
        "    print(\"Start One-Shot Test\")\n",
        "    os_err = oneshot(OneShotTestLoader, net)\n",
        "    print(\"Epoch: {}, OneShot Error: {}\".format(epoch, os_err))\n",
        "    os_errors.append(os_err)\n",
        "\n",
        "    file = open(params.best_path+'/val_errors.txt', mode='w+')\n",
        "    file.write(str(val_errors))\n",
        "    file.close()\n",
        "    file = open(params.best_path+'/train_losses.txt', mode='w+')\n",
        "    file.write(str(avg_loss))\n",
        "    file.close()\n",
        "    file = open(params.best_path+'/OneShot_acc.txt', mode='w+')\n",
        "    file.write(str(os_errors))\n",
        "    file.close()\n",
        "\n",
        "\n",
        "def OneShotTest(NumPairs=30000,  load_path='/content/gdrive/My Drive/Siamese_Net/saves/', latest = False):\n",
        "    net = SiameseNet()\n",
        "    net.cuda()\n",
        "     \n",
        "    _, _, _, OneShotTestLoader = read_data(NumPairs, one_shot=False, remake=False)\n",
        "   \n",
        "    if latest == False :\n",
        "      load_path=load_path + \"Best_\"+str(NumPairs)\n",
        "      checkpoint = torch.load(os.path.join(load_path, 'best.pt'))\n",
        "      print(\"best ckp epoch \", checkpoint['epoch'])\n",
        "      net.load_state_dict(checkpoint['state_dict'])\n",
        "    else:\n",
        "      load_path=load_path + \"Checkpoint_\"+str(NumPairs)\n",
        "      checkpoint = torch.load(os.path.join(load_path, 'checkpoint.pt'))\n",
        "      print(\"latest ckp epoch \", checkpoint['epoch'])\n",
        "      net.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "\n",
        "    # One Shot\n",
        "    net.eval()\n",
        "    print(\"Start One-Shot Test\")\n",
        "    os_err = oneshot(OneShotTestLoader, net)\n",
        "    print(\"One-Shot Test Error: {}\".format(os_err)) \n",
        "    return os_err\n",
        "\n",
        "def plotFilters(NumPairs=30000, load_path = '/content/gdrive/My Drive/Siamese_Net/saves/'):\n",
        "    net = SiameseNet()\n",
        "\n",
        "    load_path += 'Checkpoint_' + str(NumPairs)\n",
        "   \n",
        "    checkpoint = torch.load(os.path.join(load_path, 'checkpoint.pt'))\n",
        "    print(\"ckp epoch \", checkpoint['epoch'])\n",
        "    net.load_state_dict(checkpoint['state_dict'])\n",
        "\n",
        "    kernels = net.layer1[0].weight.detach().cpu()\n",
        "    kernels = kernels.view(-1, 1, 10, 10)\n",
        "    imshow(torchvision.utils.make_grid(kernels, pad_value=1, normalize=True, scale_each=True, nrow = 16))\n",
        "\n",
        "    activations = {}\n",
        "\n",
        "    def get_activation(name):\n",
        "        def hook(model, input, output):\n",
        "            activations[name] = output.detach()\n",
        "        return hook\n",
        "    \n",
        "    net.layer1[0].register_forward_hook(get_activation('layer0'))\n",
        "    net.layer1[3].register_forward_hook(get_activation('layer3'))\n",
        "    net.layer1[6].register_forward_hook(get_activation('layer6'))\n",
        "    net.layer1[9].register_forward_hook(get_activation('layer9'))\n",
        "    TrainLoader, EvalLoader, OneShotValLoader, OneShotTestLoader = read_data(NumPairs, one_shot=False, remake=False, distort=False)\n",
        "    dataiter = iter(TrainLoader)\n",
        "    img1, img2, _ = dataiter.next()\n",
        "    img1 = img1[0].view(-1, 1, 105, 105)\n",
        "    img2 = img2[0].view(-1, 1, 105, 105)\n",
        "    out = net(img1, img2)\n",
        "    act = {}\n",
        "    act[0] = activations['layer0'].view(-1, 1, 96, 96)\n",
        "    act[1] = activations['layer3'].view(-1, 1, 42, 42)\n",
        "    act[2] = activations['layer6'].view(-1, 1, 18, 18)\n",
        "    act[3] = activations['layer9'].view(-1, 1, 6, 6)\n",
        "    \n",
        "    for i in range(4):\n",
        "        imshow(torchvision.utils.make_grid(act[i], nrow = 16, pad_value=1, normalize=True, scale_each=True))\n",
        "    imshow(torchvision.utils.make_grid(img2.view(-1, 1, 105, 105), scale_each=True))\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='magma', interpolation=None)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  NumPairs = 150000\n",
        "  spath = '/content/gdrive/My Drive/Siamese_Net/saves/Affine_'\n",
        "  main(NumPairs=NumPairs, distort=True, save_path=spath)\n",
        "  print(np.mean([OneShotTest(NumPairs=NumPairs, load_path=spath, latest = True) for i in range(10)]))\n",
        "  plotFilters(NumPairs=NumPairs, load_path=spath)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}